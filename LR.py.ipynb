{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data\n",
      "[[6077.]\n",
      " [6186.]\n",
      " [4987.]\n",
      " ...\n",
      " [5479.]\n",
      " [5694.]\n",
      " [5682.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Scaler():\n",
    "    # hint: https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "    def __call__(self,features, is_train=False):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def get_features(csv_path,is_train=False,scaler=None):\n",
    "    \n",
    "    '''\n",
    "    Description:\n",
    "    read input feature columns from csv file\n",
    "    manipulate feature columns, create basis functions, do feature scaling etc.\n",
    "    return a feature matrix (numpy array) of shape m x n \n",
    "    m is number of examples, n is number of features\n",
    "    return value: numpy array\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    csv_path: path to csv file\n",
    "    is_train: True if using training data (optional)\n",
    "    scaler: a class object for doing feature scaling (optional)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    help:\n",
    "    useful links: \n",
    "        * https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "        * https://www.geeksforgeeks.org/python-read-csv-using-pandas-read_csv/\n",
    "    '''\n",
    "    '''\n",
    "    col_names = ['timedelta',\n",
    "                 'n_tokens_title',\n",
    "                 'n_tokens_content',\n",
    "                 'n_unique_tokens',\n",
    "                 'n_non_stop_words',\n",
    "                 'n_non_stop_unique_tokens',\n",
    "                 'num_hrefs',\n",
    "                 'num_self_hrefs',\n",
    "                 'num_imgs',\n",
    "                 'num_videos',\n",
    "                 'average_token_length',\n",
    "                 'num_keywords',\n",
    "                 'data_channel_is_lifestyle',\n",
    "                 'data_channel_is_entertainment',\n",
    "                 'data_channel_is_bus',\n",
    "                 'data_channel_is_socmed',\n",
    "                 'data_channel_is_tech',\n",
    "                 'data_channel_is_world',\n",
    "                 'kw_min_min',\n",
    "                 'kw_max_min',\n",
    "                 'kw_avg_min',\n",
    "                 'kw_min_max',\n",
    "                 'kw_max_max',\n",
    "                 'kw_avg_max',\n",
    "                 'kw_min_avg',\n",
    "                 'kw_max_avg',\n",
    "                 'kw_avg_avg',\n",
    "                 'self_reference_min_shares',\n",
    "                 'self_reference_max_shares',\n",
    "                 'self_reference_avg_sharess',\n",
    "                 'weekday_is_monday',\n",
    "                 'weekday_is_tuesday',\n",
    "                 'weekday_is_wednesday',\n",
    "                 'weekday_is_thursday',\n",
    "                 'weekday_is_friday',\n",
    "                 'weekday_is_saturday',\n",
    "                 'weekday_is_sunday',\n",
    "                 'is_weekend',\n",
    "                 'LDA_00',\n",
    "                 'LDA_01',\n",
    "                 'LDA_02',\n",
    "                 'LDA_03',\n",
    "                 'LDA_04',\n",
    "                 'global_subjectivity',\n",
    "                 'global_sentiment_polarity',\n",
    "                 'global_rate_positive_words',\n",
    "                 'global_rate_negative_words',\n",
    "                 'rate_positive_words',\n",
    "                 'rate_negative_words',\n",
    "                 'avg_positive_polarity',\n",
    "                 'min_positive_polarity',\n",
    "                 'max_positive_polarity',\n",
    "                 'avg_negative_polarity',\n",
    "                 'min_negative_polarity',\n",
    "                 'max_negative_polarity',\n",
    "                 'title_subjectivity',\n",
    "                 'title_sentiment_polarity',\n",
    "                 'abs_title_subjectivity',\n",
    "                 'abs_title_sentiment_polarity']\n",
    "    df_list =[]\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path, header = 1)#, names = col_names, error_bad_lines=False, index_col=False,dtype='unicode')\n",
    "    \"\"\"\n",
    "    ### manipulation ###\n",
    "    df_list.append(df[['n_tokens_title','n_tokens_content','n_unique_tokens','n_non_stop_words', 'n_non_stop_unique_tokens']])\n",
    "    df_list.append(df[['num_hrefs', 'num_self_hrefs','num_imgs','num_videos']] )\n",
    "    df_list.append(df[['data_channel_is_lifestyle', 'data_channel_is_entertainment','data_channel_is_bus','data_channel_is_socmed','data_channel_is_tech','data_channel_is_world']])\n",
    "    df_list.append(df[[ 'kw_min_min','kw_max_min','kw_avg_min','kw_min_max','kw_max_max','kw_avg_max','kw_min_avg','kw_max_avg','kw_avg_avg']])\n",
    "    df_list.append(df[['self_reference_min_shares', 'self_reference_max_shares','self_reference_avg_sharess']])\n",
    "    df_list.append(df[['weekday_is_monday','weekday_is_tuesday','weekday_is_wednesday','weekday_is_thursday','weekday_is_friday','weekday_is_saturday','weekday_is_sunday']])\n",
    "    df_list.append(df[['LDA_00', 'LDA_01','LDA_02','LDA_03','LDA_04']])\n",
    "    df_list.append(df[['global_subjectivity','global_sentiment_polarity','global_rate_positive_words','global_rate_negative_words']])\n",
    "    df_list.append(df[['rate_positive_words', 'rate_negative_words']])\n",
    "    df_list.append(df[['avg_positive_polarity','min_positive_polarity','max_positive_polarity','avg_negative_polarity', 'min_negative_polarity','max_negative_polarity' , 'title_subjectivity','title_sentiment_polarity' ,'abs_title_subjectivity','abs_title_sentiment_polarity']])\n",
    "    \"\"\"\n",
    "        \n",
    "    #### scaling ######\n",
    "    #df = (df - df.mean())/df.std()  \n",
    "    df = (df - df.min())/(df.max() -  df.min() )\n",
    "   # print(df)\n",
    "    arr = df.to_numpy()\n",
    "    #print(arr)\n",
    "    \n",
    "    \n",
    "    return arr\n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    #raise NotImplementedError\n",
    "\n",
    "   \n",
    "    \n",
    "def get_targets(csv_path):\n",
    "    '''\n",
    "    Description:\n",
    "    read target outputs from the csv file\n",
    "    return a numpy array of shape m x 1\n",
    "    m is number of examples\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path, usecols = [59])\n",
    "    arr_t = df.to_numpy()\n",
    "    print(arr_t)\n",
    "    #raise NotImplementedError\n",
    "     \n",
    "\n",
    "def analytical_solution(feature_matrix, targets, C=0.0):\n",
    "    '''\n",
    "    Description:\n",
    "    implement analytical solution to obtain weights\n",
    "    as described in lecture 5d\n",
    "    return value: numpy array\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    targets: numpy array of shape m x 1\n",
    "    '''\n",
    "\n",
    "    raise NotImplementedError \n",
    "\n",
    "def get_predictions(feature_matrix, weights):\n",
    "    '''\n",
    "    description\n",
    "    return predictions given feature matrix and weights\n",
    "    return value: numpy array\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    '''\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "def mse_loss(feature_matrix, weights, targets):\n",
    "    '''\n",
    "    Description:\n",
    "    Implement mean squared error loss function\n",
    "    return value: float (scalar)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    targets: numpy array of shape m x 1\n",
    "    '''\n",
    "    raise NotImplementedError\n",
    "\n",
    "def l2_regularizer(weights):\n",
    "    '''\n",
    "    Description:\n",
    "    Implement l2 regularizer\n",
    "    return value: float (scalar)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments\n",
    "    weights: numpy array of shape n x 1\n",
    "    '''\n",
    "    raise NotImplementedError\n",
    "\n",
    "def loss_fn(feature_matrix, weights, targets, C=0.0):\n",
    "    '''\n",
    "    Description:\n",
    "    compute the loss function: mse_loss + C * l2_regularizer\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    targets: numpy array of shape m x 1\n",
    "    C: weight for regularization penalty\n",
    "    return value: float (scalar)\n",
    "    '''\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "def compute_gradients(feature_matrix, weights, targets, C=0.0):\n",
    "    '''\n",
    "    Description:\n",
    "    compute gradient of weights w.r.t. the loss_fn function implemented above\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    targets: numpy array of shape m x 1\n",
    "    C: weight for regularization penalty\n",
    "    return value: numpy array\n",
    "    '''\n",
    "    raise NotImplementedError\n",
    "\n",
    "def sample_random_batch(feature_matrix, targets, batch_size):\n",
    "    '''\n",
    "    Description\n",
    "    Batching -- Randomly sample batch_size number of elements from feature_matrix and targets\n",
    "    return a tuple: (sampled_feature_matrix, sampled_targets)\n",
    "    sampled_feature_matrix: numpy array of shape batch_size x n\n",
    "    sampled_targets: numpy array of shape batch_size x 1\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    targets: numpy array of shape m x 1\n",
    "    batch_size: int\n",
    "    '''    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "def initialize_weights(n):\n",
    "    '''\n",
    "    Description:\n",
    "    initialize weights to some initial values\n",
    "    return value: numpy array of shape n x 1\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments\n",
    "    n: int\n",
    "    '''\n",
    "    raise NotImplementedError\n",
    "\n",
    "def update_weights(weights, gradients, lr):\n",
    "    '''\n",
    "    Description:\n",
    "    update weights using gradient descent\n",
    "    retuen value: numpy matrix of shape nx1\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    # weights: numpy matrix of shape nx1\n",
    "    # gradients: numpy matrix of shape nx1\n",
    "    # lr: learning rate\n",
    "    '''    \n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "def early_stopping(arg_1=None, arg_2=None, arg_3=None, arg_n=None):\n",
    "    # allowed to modify argument list as per your need\n",
    "    # return True or False\n",
    "    raise NotImplementedError\n",
    "    \n",
    "\n",
    "def do_gradient_descent(train_feature_matrix,  \n",
    "                        train_targets, \n",
    "                        dev_feature_matrix,\n",
    "                        dev_targets,\n",
    "                        lr=1.0,\n",
    "                        C=0.0,\n",
    "                        batch_size=32,\n",
    "                        max_steps=10000,\n",
    "                        eval_steps=5):\n",
    "    '''\n",
    "    feel free to significantly modify the body of this function as per your needs.\n",
    "    ** However **, you ought to make use of compute_gradients and update_weights function defined above\n",
    "    return your best possible estimate of LR weights\n",
    "\n",
    "    a sample code is as follows -- \n",
    "    '''\n",
    "    weights = initialize_weights(n)\n",
    "    dev_loss = mse_loss(dev_feature_matrix, weights, dev_targets)\n",
    "    train_loss = mse_loss(train_feature_matrix, weights, train_targets)\n",
    "\n",
    "    print(\"step {} \\t dev loss: {} \\t train loss: {}\".format(0,dev_loss,train_loss))\n",
    "    for step in range(1,max_steps+1):\n",
    "\n",
    "        #sample a batch of features and gradients\n",
    "        features,targets = sample_random_batch(train_feature_matrix,train_targets,batch_size)\n",
    "        \n",
    "        #compute gradients\n",
    "        gradients = compute_gradients(features, weights, targets, C)\n",
    "        \n",
    "        #update weights\n",
    "        weights = update_weights(weights, gradients, lr)\n",
    "\n",
    "        if step%eval_steps == 0:\n",
    "            dev_loss = mse_loss(dev_feature_matrix, weights, dev_targets)\n",
    "            train_loss = mse_loss(train_feature_matrix, weights, train_targets)\n",
    "            print(\"step {} \\t dev loss: {} \\t train loss: {}\".format(step,dev_loss,train_loss))\n",
    "\n",
    "        '''\n",
    "        implement early stopping etc. to improve performance.\n",
    "        '''\n",
    "\n",
    "    return weights\n",
    "\n",
    "def do_evaluation(feature_matrix, targets, weights):\n",
    "    # your predictions will be evaluated based on mean squared error \n",
    "    predictions = get_predictions(feature_matrix, weights)\n",
    "    loss =  mse_loss(feature_matrix, weights, targets)\n",
    "    return loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #scaler = Scaler() #use of scaler is optional\n",
    "    print(\"training data\")\n",
    "    train_features, train_targets =  get_features('data/train.csv',True), get_targets('data/train.csv')\n",
    "    #train_features, train_targets = get_features('data/train.csv',True), get_targets('data/train.csv')\n",
    "    #print(\"dev data\")\n",
    "    #dev_features, dev_targets = get_features('data/dev.csv',False), get_targets('data/dev.csv')\n",
    "    \"\"\"\n",
    "    \n",
    "    a_solution = analytical_solution(train_features, train_targets, C=1e-8)\n",
    "    print('evaluating analytical_solution...')\n",
    "    dev_loss=do_evaluation(dev_features, dev_targets, a_solution)\n",
    "    train_loss=do_evaluation(train_features, train_targets, a_solution)\n",
    "    print('analytical_solution \\t train loss: {}, dev_loss: {} '.format(train_loss, dev_loss))\n",
    "\n",
    "    print('training LR using gradient descent...')\n",
    "    gradient_descent_soln = do_gradient_descent(train_features, \n",
    "                        train_targets, \n",
    "                        dev_features,\n",
    "                        dev_targets,\n",
    "                        lr=1.0,\n",
    "                        C=0.0,\n",
    "                        batch_size=32,\n",
    "                        max_steps=2000000,\n",
    "                        eval_steps=5)\n",
    "\n",
    "    print('evaluating iterative_solution...')\n",
    "    dev_loss=do_evaluation(dev_features, dev_targets, gradient_descent_soln)\n",
    "    train_loss=do_evaluation(train_features, train_targets, gradient_descent_soln)\n",
    "    print('gradient_descent_soln \\t train loss: {}, dev_loss: {} '.format(train_loss, dev_loss))\n",
    "    \n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
